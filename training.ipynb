{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCa_5_BrUTkQ"
      },
      "source": [
        "## Game Source Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeDtTDUuUTkX",
        "outputId": "0eee6df7-b345-4f53-807c-2d1bbd154041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb # Install Xvfb\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "\n",
        "import turtle\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from pyvirtualdisplay import Display # Import Display\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Create virtual display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start() # Start the virtual display\n",
        "\n",
        "\n",
        "GAME_SPEED = 0.5 # MS\n",
        "ACTIONS = [1,2,3,4]\n",
        "google_drive_weights_dir = '/content/drive/MyDrive/SnakeRLAgent/'\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class Head:\n",
        "    def __init__(self):\n",
        "        self.head = turtle.Turtle();\n",
        "        self.head.speed(0)\n",
        "        self.head.shape(\"square\")\n",
        "        self.head.color(\"black\")\n",
        "        self.head.penup()\n",
        "        self.head.goto(0,0)\n",
        "        self.head.direction = \"stop\"\n",
        "\n",
        "    def go_up(self):\n",
        "        if self.head.direction != \"down\":\n",
        "            self.head.direction = \"up\"\n",
        "\n",
        "    def go_down(self):\n",
        "        if self.head.direction != \"up\":\n",
        "            self.head.direction = \"down\"\n",
        "\n",
        "    def go_left(self):\n",
        "        if self.head.direction != \"right\":\n",
        "            self.head.direction = \"left\"\n",
        "\n",
        "    def go_right(self):\n",
        "        if self.head.direction != \"left\":\n",
        "            self.head.direction = \"right\"\n",
        "\n",
        "    def move(self):\n",
        "        if self.head.direction == \"up\":\n",
        "            y = self.head.ycor()\n",
        "            self.head.sety(y + 20)\n",
        "\n",
        "        if self.head.direction == \"down\":\n",
        "            y = self.head.ycor()\n",
        "            self.head.sety(y - 20)\n",
        "\n",
        "        if self.head.direction == \"left\":\n",
        "            x = self.head.xcor()\n",
        "            self.head.setx(x - 20)\n",
        "\n",
        "        if self.head.direction == \"right\":\n",
        "            x = self.head.xcor()\n",
        "            self.head.setx(x + 20)\n",
        "\n",
        "class GameLauncher:\n",
        "    def __init__(self):\n",
        "        self.turtle = turtle\n",
        "        self.boundary = 140  # Game boundary is Â±140\n",
        "        self.cell_size = 20  # Each block in the snake segment is 20 x 20\n",
        "\n",
        "        # Calculate grid size from boundaries and cell size\n",
        "        # (-140 to +140 = 280 units total, divided by 20 = 14 cells)\n",
        "        self.grid_size = (self.boundary * 2) // self.cell_size  # Equals 14``\n",
        "\n",
        "        self.setup_window()\n",
        "        self.segments = []\n",
        "        self.high_score = 0\n",
        "        self.head = Head()\n",
        "        self.setup_pen()\n",
        "        self.setup_food()\n",
        "        self.score = 0\n",
        "\n",
        "    def quit(self):\n",
        "        self.turtle.reset()\n",
        "\n",
        "\n",
        "    def check_wall_collision(self):\n",
        "        head = self.head\n",
        "        segments = self.segments\n",
        "        if head.head.xcor()>140 or head.head.xcor()<-140 or head.head.ycor()>140 or head.head.ycor()<-140:\n",
        "            time.sleep(1)\n",
        "            head.head.goto(0,0)\n",
        "            head.head.direction = \"stop\"\n",
        "\n",
        "            # Hide the segments\n",
        "            for segment in segments:\n",
        "                segment.goto(1000, 1000)\n",
        "\n",
        "            # Clear the segments list\n",
        "            segments.clear()\n",
        "\n",
        "            # Reset the score\n",
        "            self.score = 0\n",
        "\n",
        "            # Reset the delay\n",
        "            self.delay = GAME_SPEED\n",
        "\n",
        "            self.pen.clear()\n",
        "            self.pen.write(\"Score: {}  High Score: {}\".format(self.score, self.high_score), align=\"center\", font=(\"Courier\", 24, \"normal\"))\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def check_food_collision(self):\n",
        "        if self.head.head.distance(self.food) < 20:\n",
        "            # Move the self.food to a random spot\n",
        "            x = random.randint(-140, 140)\n",
        "            y = random.randint(-140, 140)\n",
        "            self.food.goto(x,y)\n",
        "\n",
        "            # Add a segment\n",
        "            new_segment = turtle.Turtle()\n",
        "            new_segment.speed(0)\n",
        "            new_segment.shape(\"square\")\n",
        "            new_segment.color(\"grey\")\n",
        "            new_segment.penup()\n",
        "            self. segments.append(new_segment)\n",
        "\n",
        "            # Shorten the delay\n",
        "            self.delay -= 0.001\n",
        "\n",
        "            # Increase the score\n",
        "            self.score += 10\n",
        "\n",
        "            if self.score > self.high_score:\n",
        "                self.high_score = self.score\n",
        "\n",
        "            self.pen.clear()\n",
        "            self.pen.write(\"Score: {}  High Score: {}\".format(self.score, self.high_score), align=\"center\", font=(\"Courier\", 24, \"normal\"))\n",
        "\n",
        "    def state_to_array(self):\n",
        "        def turtle_to_grid(x, y):\n",
        "            # Convert from (-140, 140) range to (0, 13) range\n",
        "            grid_x = int((x + self.boundary) // self.cell_size)\n",
        "            grid_y = int((y + self.boundary) // self.cell_size)\n",
        "            # Ensure coordinates are within grid bounds\n",
        "            grid_x = max(0, min(grid_x, self.grid_size - 1))\n",
        "            grid_y = max(0, min(grid_y, self.grid_size - 1))\n",
        "            return grid_x, grid_y\n",
        "\n",
        "        # Create empty grid\n",
        "        grid = np.zeros((self.grid_size, self.grid_size, 3))\n",
        "\n",
        "        # Set head\n",
        "        head_x, head_y = turtle_to_grid(self.head.head.xcor(), self.head.head.ycor())\n",
        "        grid[head_y, head_x, 0] = 1\n",
        "\n",
        "        # Set body segments\n",
        "        for segment in self.segments:\n",
        "            seg_x, seg_y = turtle_to_grid(segment.xcor(), segment.ycor())\n",
        "            grid[seg_y, seg_x, 1] = 1\n",
        "\n",
        "        # Set food\n",
        "        food_x, food_y = turtle_to_grid(self.food.xcor(), self.food.ycor())\n",
        "        grid[food_y, food_x, 2] = 1  # Make sure this line is actually setting the value\n",
        "\n",
        "\n",
        "        return torch.from_numpy(grid.flatten()).float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_snake_body(self):\n",
        "        # Move the end segments first in reverse order\n",
        "        for index in range(len(self.segments)-1, 0, -1):\n",
        "            x = self.segments[index-1].xcor()\n",
        "            y = self.segments[index-1].ycor()\n",
        "            self.segments[index].goto(x, y)\n",
        "\n",
        "        # Move segment 0 to where the head is\n",
        "        if len(self.segments) > 0:\n",
        "            x = self.head.head.xcor()\n",
        "            y = self.head.head.ycor()\n",
        "            self.segments[0].goto(x,y)\n",
        "\n",
        "\n",
        "    def get_random_action(self):\n",
        "        # This is a placeholder. In a real RL setup, you'd get the action from your model\n",
        "        return random.choice(ACTIONS)\n",
        "\n",
        "    def get_reward(self, old_score, new_score, collision):\n",
        "        if collision:\n",
        "            return -1\n",
        "        elif new_score > old_score:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def step(self,action):\n",
        "        old_score = self.score\n",
        "        head = self.head\n",
        "        self.delay = GAME_SPEED\n",
        "\n",
        "        # Score\n",
        "        segments = self.segments\n",
        "\n",
        "        match action:\n",
        "            case 0:\n",
        "                self.head.go_up()\n",
        "            case 1:\n",
        "                self.head.go_down()\n",
        "            case 2:\n",
        "                self.head.go_left()\n",
        "            case _:\n",
        "                self.head.go_right()\n",
        "\n",
        "        self.wn.update()\n",
        "        collision_check = self.check_wall_collision() or self.check_self_collision()\n",
        "        self.check_food_collision()\n",
        "        self.update_snake_body()\n",
        "        head.move()\n",
        "        new_state = self.state_to_array()\n",
        "        reward = self.get_reward(old_score, self.score, collision_check)\n",
        "        done = collision_check\n",
        "\n",
        "        return new_state,reward,done\n",
        "\n",
        "    def check_self_collision(self):\n",
        "        segments = self.segments\n",
        "        head = self.head\n",
        "        pen = self.pen\n",
        "\n",
        "    # Check for head collision with the body segments\n",
        "        for segment in segments:\n",
        "            if segment.distance(head.head) < 20:\n",
        "                time.sleep(1)\n",
        "                head.head.goto(0,0)\n",
        "                head.direction = \"stop\"\n",
        "\n",
        "                # Hide the segments\n",
        "                for segment in segments:\n",
        "                    segment.goto(1000, 1000)\n",
        "\n",
        "                # Clear the segments list\n",
        "                segments.clear()\n",
        "\n",
        "                # Reset the score\n",
        "                self.score = 0\n",
        "\n",
        "                # Reset the delay\n",
        "                self.delay = 0.1\n",
        "\n",
        "                # Update the score display\n",
        "                pen.clear()\n",
        "                pen.write(\"Score: {}  High Score: {}\".format(self.score, self.high_score), align=\"center\", font=(\"Courier\", 24, \"normal\"))\n",
        "\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def setup_window(self):\n",
        "        self.wn = self.turtle.Screen()\n",
        "        self.wn = turtle.Screen()\n",
        "        self.wn.title(\"SnaKE\")\n",
        "        self.wn.bgcolor(\"green\")\n",
        "        self.wn.setup(width=300, height=300)\n",
        "        self.wn.tracer(0) # Turns off the screen updates\n",
        "\n",
        "    def setup_pen(self):\n",
        "        self.pen = self.turtle.Turtle()\n",
        "        self.pen.speed(0)\n",
        "        self.pen.shape(\"square\")\n",
        "        self.pen.color(\"white\")\n",
        "        self.pen.penup()\n",
        "        self.pen.hideturtle()\n",
        "        self.pen.goto(0, 140)\n",
        "        self.pen.write(\"Score: 0  High Score: 0\", align=\"center\", font=(\"Courier\", 24, \"normal\"))\n",
        "\n",
        "    def setup_food(self):\n",
        "        self.food = turtle.Turtle()\n",
        "        self.food.speed(0)\n",
        "        self.food.shape(\"circle\")\n",
        "        self.food.color(\"red\")\n",
        "        self.food.penup()\n",
        "        self.food.goto(0,100)\n",
        "\n",
        "\n",
        "# game = GameLauncher()\n",
        "# for i in range(20):\n",
        "#     action = game.get_random_action()\n",
        "#     new_state,reward,done =game.step(action)\n",
        "#     print(f\"Run {new_state.shape} |Reward is {reward} | Done is {done}\")\n",
        "#     time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlN9Wi8ZUTkc"
      },
      "source": [
        "# Model and Replay Buffer definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KricauJzUTkd"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "class ReplayBuffer:\n",
        "    def __init__(self,max_capacity):\n",
        "        self.memory = deque([],max_capacity)\n",
        "        self.device = torch.device(\"cuda\")\n",
        "\n",
        "    # Takes a named tuple of Transition\n",
        "    def push(self,transition_):\n",
        "        self.memory.append(transition_)\n",
        "\n",
        "    def can_sample(self,batch_size):\n",
        "        return len(self.memory) >= batch_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "            \"\"\"Sample a batch of transitions and convert to torch tensors\"\"\"\n",
        "            if not self.can_sample(batch_size):\n",
        "                raise ValueError(f\"Not enough samples in buffer. Has {len(self)} samples, but {batch_size} requested\")\n",
        "\n",
        "            # Sample random transitions\n",
        "            transitions = random.sample(list(self.memory), batch_size)\n",
        "\n",
        "            # Transpose the batch\n",
        "            batch = Transition(*zip(*transitions))\n",
        "\n",
        "            try:\n",
        "                # Convert states to tensor\n",
        "                state_batch = torch.stack([\n",
        "                    torch.as_tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s\n",
        "                    for s in batch.state\n",
        "                ]).to(self.device)\n",
        "\n",
        "                # Convert actions to tensor - handle scalar integers\n",
        "                action_batch = torch.tensor(\n",
        "                    [a if isinstance(a, (list, tuple)) else [a] for a in batch.action],\n",
        "                    dtype=torch.long\n",
        "                ).to(self.device)\n",
        "\n",
        "                # Convert next_states to tensor\n",
        "                next_state_batch = torch.stack([\n",
        "                    torch.as_tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s\n",
        "                    for s in batch.next_state\n",
        "                ]).to(self.device)\n",
        "\n",
        "                # Convert rewards to tensor - handle scalar values\n",
        "                reward_batch = torch.tensor(\n",
        "                    [r if isinstance(r, (list, tuple)) else [r] for r in batch.reward],\n",
        "                    dtype=torch.float32\n",
        "                ).to(self.device)\n",
        "\n",
        "                return state_batch, action_batch, next_state_batch, reward_batch\n",
        "\n",
        "            except Exception as e:\n",
        "                # Detailed error reporting\n",
        "                shapes = {\n",
        "                    'state': [np.shape(s) for s in batch.state[:3]],  # Show first 3 shapes\n",
        "                    'action': [np.shape(a) if hasattr(a, 'shape') else type(a) for a in batch.action[:3]],\n",
        "                    'next_state': [np.shape(s) for s in batch.next_state[:3]],\n",
        "                    'reward': [np.shape(r) if hasattr(r, 'shape') else type(r) for r in batch.reward[:3]]\n",
        "                }\n",
        "                raise RuntimeError(f\"Error creating batch: {str(e)}\\n\"\n",
        "                                f\"Sample shapes/types:\\n\"\n",
        "                                f\"States: {shapes['state']}\\n\"\n",
        "                                f\"Actions: {shapes['action']}\\n\"\n",
        "                                f\"Next States: {shapes['next_state']}\\n\"\n",
        "                                f\"Rewards: {shapes['reward']}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,n_observations,actions):\n",
        "        super(DQN,self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input = nn.Linear(n_observations,128)\n",
        "        self.middle = nn.Linear(128,256)\n",
        "        self.fc = nn.Linear(256,len(actions))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.input(x))\n",
        "        x = self.relu(self.middle(x))\n",
        "        return self.fc(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybq9eZM6UTke"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOhgriESUTke",
        "outputId": "658be553-1bcd-4c49-d681-8dde56f41bfd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 500 done with average error | 0.03655852170454131\n",
            "Episode 520 done with average error | 0.021030746400356293\n",
            "Episode 540 done with average error | 0.0033184775975038947\n",
            "Episode 560 done with average error | 0.005861581582576036\n",
            "Episode 580 done with average error | 0.001640467477651934\n",
            "Episode 600 done with average error | 0.003905514249874448\n",
            "Episode 620 done with average error | 0.014561689272522927\n",
            "Episode 640 done with average error | 0.020022435171995312\n",
            "Episode 660 done with average error | 0.019433296906451385\n",
            "Episode 680 done with average error | 0.04493850383620996\n",
            "Episode 700 done with average error | 0.02002422528510744\n",
            "Episode 720 done with average error | 0.02671019013293765\n",
            "Episode 740 done with average error | 0.06912805377082391\n",
            "Episode 760 done with average error | 0.07287544467382961\n",
            "Episode 780 done with average error | 0.06571346626657507\n",
            "Episode 800 done with average error | 0.0855372091755271\n",
            "Episode 820 done with average error | 0.1738412781403615\n",
            "Episode 840 done with average error | 0.10867552112787962\n",
            "Episode 860 done with average error | 0.14114195510352912\n",
            "Episode 880 done with average error | 0.16981495341614766\n",
            "Episode 900 done with average error | 0.0071326118770711384\n",
            "Episode 920 done with average error | 0.02787818357362138\n",
            "Episode 940 done with average error | 0.25584228671240533\n",
            "Episode 960 done with average error | 0.21620655725044863\n",
            "Episode 980 done with average error | 0.3444462368885676\n",
            "Episode 1000 done with average error | 0.2748041406951167\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.functional import F\n",
        "import copy\n",
        "epsilon = 1.0       # Start epsilon at 1.0 for exploration\n",
        "epsilon_min = 0.01  # Minimum epsilon for a reasonable amount of exploitation\n",
        "epsilon_decay = 0.98\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "def policy(state,action_list,inference_model):\n",
        "    global epsilon\n",
        "    if torch.rand(1) < epsilon:\n",
        "        epsilon = max(epsilon_min,epsilon * epsilon_decay)\n",
        "        return random.randrange(len(action_list))\n",
        "    else:\n",
        "        return inference_model(state).detach().argmax().item()\n",
        "\n",
        "\n",
        "\n",
        "def training_model(policy_net:DQN,game_instance:GameLauncher,lr,batch_size,episodes = 20,gamma=0.99,actions=[1,2,3,4]):\n",
        "    optimizer = torch.optim.AdamW(policy_net.parameters(),lr=lr,)\n",
        "    optimizer.load_state_dict(torch.load(f\"{google_drive_weights_dir}/optimizer.pth\"))\n",
        "    replay_buffer = ReplayBuffer(max_capacity=batch_size + 1)\n",
        "    target_q_model = copy.deepcopy(policy_net).to(device).eval()\n",
        "\n",
        "    for episode in range(1,episodes + 1):\n",
        "        done = False\n",
        "        total_loss = 0\n",
        "        num_updates = 0  # Track number of updates for averaging\n",
        "        while not done:\n",
        "            current_state = game_instance.state_to_array().to(device)\n",
        "            action = policy(state=current_state,action_list=actions,inference_model=target_q_model)\n",
        "            new_state,reward,done =game_instance.step(action)\n",
        "\n",
        "            replay_buffer.push(Transition(state=current_state,action=action,next_state=new_state,reward=reward))\n",
        "\n",
        "            if replay_buffer.can_sample(batch_size):\n",
        "                state_b,action_b,next_state_b,reward_b = replay_buffer.sample(batch_size=batch_size)\n",
        "                q_values = policy_net(state_b)\n",
        "\n",
        "                current_q_value = q_values.gather(1, action_b)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    maximum_next_q_value = torch.max(target_q_model(next_state_b),dim=1,keepdim=True)[0]\n",
        "                    target_q_value =  reward_b + (gamma * maximum_next_q_value * (1-done))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                criterion = nn.SmoothL1Loss()\n",
        "\n",
        "                loss = criterion(current_q_value,target_q_value)\n",
        "                loss.backward() # compute gradients for all parameters\n",
        "                total_loss += loss.item()\n",
        "                num_updates += 1  # Track number of updates\n",
        "\n",
        "\n",
        "                torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "        if episode % 20 == 0 and num_updates != 0 and replay_buffer.can_sample(batch_size):\n",
        "            target_q_model.load_state_dict(policy_net.state_dict())\n",
        "            print(f\"Episode {episode} done with average error | {total_loss / num_updates}\")\n",
        "            total_loss = 0\n",
        "            torch.save(policy_net.state_dict(),f\"{google_drive_weights_dir}/policy_episode({episode}).pth\")\n",
        "            torch.save(optimizer.state_dict(),f\"{google_drive_weights_dir}/optimizer.pth\")\n",
        "\n",
        "\n",
        "game = GameLauncher()\n",
        "policy_net = DQN(len(torch.rand(588,)),actions=ACTIONS).to(device)\n",
        "policy_net.load_state_dict(torch.load(f\"{google_drive_weights_dir}/policy_episode(1640).pth\"))\n",
        "training_model(policy_net=policy_net,game_instance=game,lr=0.001,batch_size=2_500,episodes=1_000)\n",
        "# game.quit()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGe1ye_1UTkh"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57JHbjFAUTkh",
        "outputId": "97670121-657a-4f33-a53b-1da0c41ca4fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GameLauncher' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b673d3f0aac7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGameLauncher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m588\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{google_drive_weights_dir}/policy_episode(240).pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GameLauncher' is not defined"
          ]
        }
      ],
      "source": [
        "game = GameLauncher()\n",
        "model = DQN(len(torch.rand(588,)),actions=ACTIONS)\n",
        "model.load_state_dict(torch.load(f\"{google_drive_weights_dir}/policy_episode(240).pth\"))\n",
        "model.eval()\n",
        "for i in range(1000):\n",
        "    with torch.no_grad():\n",
        "        q_values = model(game.state_to_array())\n",
        "        action =  torch.argmax(q_values).item()\n",
        "    new_state,reward,done =game.step(action)\n",
        "    print(f\"Run {new_state.shape} |Reward is {reward} | Done is {done}\")\n",
        "    time.sleep(.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c35k_K6AUTki"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}