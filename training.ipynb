{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Replay Buffer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,max_capacity):\n",
    "        self.memory = deque([],max_capacity)\n",
    "        \n",
    "    # Takes a named tuple of Transition\n",
    "    def push(self,transition_):\n",
    "        self.memory.append(transition_)\n",
    "        \n",
    "    def can_sample(self,batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)        \n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,n_observations,actions):\n",
    "        self.input = nn.Linear(n_observations,128)\n",
    "        self.middle = nn.Linear(128,256)\n",
    "        self.fc = nn.Linear(256,actions)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.ReLU(self.input(x))\n",
    "        x = nn.ReLU(self.middle(x))        \n",
    "        return self.fc(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "from game.game_launch import GameLauncher\n",
    "epsilon = 1.0       # Start epsilon at 1.0 for exploration\n",
    "epsilon_min = 0.01  # Minimum epsilon for a reasonable amount of exploitation\n",
    "epsilon_decay = 0.995\n",
    "action_list = ['up', 'down', 'left', 'right']\n",
    "\n",
    "def policy(state,action_list,inference_model):\n",
    "    global epsilon  # Ensure epsilon is tracked across calls\n",
    "    if torch.rand(1) < epsilon:\n",
    "        epsilon = max(epsilon_min,epsilon * epsilon_decay)\n",
    "        return random.randrange(len(action_list))\n",
    "    else:\n",
    "        return inference_model(state).detach().argmax().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "def training_model(policy_net:DQN,target_q_model:DQN,game_instance:GameLauncher,lr,batch_size,episodes = 20,gamma=0.99,):\n",
    "    optimizer = torch.optim.AdamW(policy_net.parameters(),lr=lr,)\n",
    "    replay_buffer = ReplayBuffer(max_capacity=300)\n",
    "    for episode in range(1,episodes + 1):\n",
    "        done = False\n",
    "        total_loss = 0\n",
    "        while not done:\n",
    "            current_state = game_instance.state_to_array()\n",
    "            action = policy(state=current_state,action_list=action_list,inference_model=target_q_model)\n",
    "            new_state,reward,done =game_instance.step(action)\n",
    "            print(f\"New state is {new_state} | Reward is {reward} | Done {done} \")\n",
    "            replay_buffer.push(Transition(state=current_state,action=action,next_state=new_state,reward=reward))\n",
    "            \n",
    "            if replay_buffer.can_sample(batch_size):\n",
    "                state_b,action_b,next_state_b,reward_b = replay_buffer.sample()\n",
    "                action_pred_b = policy_net(state_b).gather(1, action_b)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    maximum_next_q_value = torch.max(target_q_model(next_state_b),dim=1,keepdim=True)[0]\n",
    "                    target_q_value =  reward_b + (gamma * maximum_next_q_value * reward_b)\n",
    "                \n",
    "                criterion = nn.SmoothL1Loss()\n",
    "                \n",
    "                loss = criterion(action_b,target_q_value)\n",
    "                loss.backward() # compute gradients for all parameters\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()\n",
    "        print(f\"Episode {episode} done with average error | {total_loss / len(replay_buffer)}\")\n",
    "        target_q_model.load_state_dict(policy_net.state_dict())\n",
    "        torch.save(f\"policy_episode({episode}).pth\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SnakeGameAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
